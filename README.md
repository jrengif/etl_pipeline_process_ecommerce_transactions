# E-Commerce Transactions Processing Project

This project aims to simulate the batch processing of transactions generated by an e-commerce platform. The data source that will act as our e-commerce service is the Online Retail Dataset - UC Irvine ([Check Raw Metada Documentation](project_documentation/data_modeling/raw_metada_doc.md)). 

Using a containerized architecture involving Docker, Airflow, Python, DBT, and PostgreSQL (staging layer and OLAP layer), the goal is to transform this source data into an analytical model that enables the organization to make informed decisions.

## Architecture Overview

This architecture was designed to run in a local environment using Docker to set up the different services it comprises.

For more information about the architecure components ([Check Solution Architecture Documentation](project_documentation/architechture_documentation/solution_architecture.md)).

The processing pipeline involved in the architecture is the next:

### Processing Pipeline

0. **Trigger Dags Manually (Optional):** An Engineer can trigger manually the Dag developed using Arflow Webserver to start the execution.

1. **Extract Data from source:** An airflow task extract data from UC Irvine repositories.

2. **Load Raw Data:** After a simple preprocessing process of the data obtained in the last step, An airflow task load the raw data into the PostgreSQL stating area waiting to be transformed.

3. **Start Transformation:** An ariflow task also is in charged of start the transformation process invoking the dbt container.

4. **Extract & Transform:** DBT transforms the data in the staging layer into an analitical model.

5. **Load Transformed Data:** DBT ingest data into OLAP layer. This data can be used now for BI, analitical purposes and decision making.

<img src="project_documentation/architechture_documentation/solution_architechture.png" alt="Solution Architecture" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">



## OLAP Layer - Star Model

The raw data is transformed into an star schema thats going to be stored in the postgres OLAP Layer. This new denormalized model allows for more agile queries that will support the company in the process of becoming a data-driven company.

For more information [Check OLAP Data Model & Metadata Documentation](project_documentation/data_modeling/olap_metadata_doc.md)

<img src="project_documentation/data_modeling/OLAP_star_model.png" alt="OLAP Star Model" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">

## Repository Structure

## Folder Structure

### `airflow/`
Contains all necessary files for setting up and configuring Apache Airflow, including:
- **DAGs** (Directed Acyclic Graphs): Defines the ETL workflows.
- **Logs**: Airflow log files.
- **requirements.txt**: Python dependencies for Airflow.

### `dbt/`
Contains the DBT project responsible for managing data transformations:
- **models/**: DBT SQL models that define the data transformation logic.
- **profiles.yml**: Configuration for connecting to the database.


### `postgres/`
Contains SQL scripts and configuration files for setting up PostgreSQL databases:
- **init.sql.template**: SQL scripts for initializing staging and OLAP databases.
- **init_sql_template.sh**: Shell script to set up the database environment..

### `project_documentation/`
A directory containing project-related documentation:
- Architecture diagrams.
- Workflow descriptions.
- User and technical manuals.

## Root Files

- **`.gitignore`**  
  Specifies which files and directories to exclude from version control, such as logs, environment files, and virtual environments.

- **`Dockerfile`**  
  Dockerfile for building the base image for the Airflow container, ensuring all dependencies are installed and configured.

- **`Dockerfile.dbt`**  
  Custom Dockerfile for building the DBT container, isolating DBT-specific dependencies.

- **`Makefile`**  
  A makefile to automate common tasks, including:
  - Building Docker images.
  - Running or cleaning up containers.
  - Checking dependencies like Python and Docker installations.

- **`README.md`**  
  The primary documentation for the project, including:
  - Overview of the repository.
  - Setup and installation instructions.
  - Usage details.

- **`docker-compose.yml`**  
  Defines and configures the services required to run the pipeline, including:
  - Airflow webserver and scheduler.
  - DBT container.
  - PostgreSQL containers for staging and OLAP layers.

- **`pre-commit-config.yaml`**  
  Configuration for pre-commit hooks, ensuring code quality standards such as linting and formatting.

- **`requirements.txt`**  
  A list of Python dependencies required for the project, ensuring a consistent environment across all contributors.

## CI/CD Approach

A Makefile was generated to provide a set of targets for managing Docker Compose deployments in a Python and Docker environment. It simplifies tasks such as checking for required dependencies (Python and Docker), building and starting Docker containers, running DBT tasks, and cleaning up unused Docker resources. It is particularly useful for automating deployment pipelines in a Continuous Integration/Continuous Deployment (CI/CD) environment.

Notes: you have to det up a linux windows subsystem fisrt if you are working with windows

### Key Components and Purpose

1. **Automated Dependency Checks**:
   - Ensures that the required dependencies (Python and Docker) are installed before starting the build process, preventing errors from missing dependencies.

2. **Automated Builds and Deployments**:
   - The `build`, `up`, `restart`, and `rebuild` targets help automate the process of building, deploying, and restarting services, which are key steps in any CI/CD pipeline. This eliminates the need for manual intervention and reduces errors during deployment.

3. **System Cleanup**:
   - The `clean` and `stop` targets help in cleaning up unused Docker resources, ensuring that system resources are not wasted by lingering containers, networks, or volumes, which is critical for maintaining a clean and efficient environment during continuous deployment.

4. **Real-time Monitoring**:
   - The `logs` target allows the real-time monitoring of services, which is important for debugging and ensuring the services are functioning as expected during the CI/CD process.

5. **DBT Task Execution**:
   - The `dbt` target ensures that data transformation tasks can be executed within the Docker environment, making the deployment of data-related tasks part of the CI/CD pipeline.

### Example Targets

```bash
# Build the Docker images and start the services
make build   # This will build the Docker images defined in the Makefile and start the services.
# Restart services if needed
make restart  # Restarts the services, useful when you need to apply changes without fully stopping.
# Stop the services
make stop     # Stops all running services, which can be used to pause or shut down your environment.
# Clean up Docker resources
make clean    # Removes unused Docker resources such as stopped containers, networks, and dangling images.
# View logs for services
make logs     # Displays the logs of the running services to help you troubleshoot or monitor service output.
# Run DBT tasks
make dbt      # Runs the DBT (Data Build Tool) tasks defined in the Makefile, typically for transforming data.
```

## Suggested architecture changes for real-world implementations

This solution was designed to run in a local environment, which is suitable for testing and development purposes. However, for a real-world implementation, this setup might not be the most appropriate. A cloud-based environment using serverless services on AWS could significantly enhance scalability, reliability, and maintainability. 

### Suggested Cloud Architecture:

- **Orchestration:**  
  Utilize **Amazon Managed Workflows for Apache Airflow (MWAA)** for orchestrating workflows. MWAA provides a fully managed service for Apache Airflow, reducing the operational overhead of managing Airflow infrastructure.

- **Data Lakes:**  
  Replace the local file storage with **AWS S3** for scalable, durable, and cost-effective storage of raw and processed data.

- **Computing:**  
  Leverage **AWS Glue Jobs** for data processing and transformations. AWS Glue supports **PySpark**, which is ideal for heavy workloads, offering distributed processing and integration with other AWS services.

- **Data Warehouse:**  
  Migrate to **Amazon Redshift** for analytics and business intelligence use cases. Redshift provides a fully managed, petabyte-scale data warehouse solution with powerful querying capabilities.

### Data Quality and Monitoring

- **Data Validation Framework:**  
  Incorporate a framework like **Great Expectations** to enforce data quality checks and validations. This ensures the integrity and reliability of your data pipeline.

- **Data Quality Monitoring:**  
  Use services like **Amazon QuickSight** to visualize and monitor data quality metrics. Dashboards can provide insights into validation results, anomalies, and overall pipeline health.

### Benefits of This Approach

- Reduced operational complexity with serverless and managed services.
- Scalability to handle larger workloads and datasets.
- Enhanced reliability, as cloud services provide fault tolerance and high availability.
- Seamless integration of data quality and monitoring tools for proactive issue detection.

This approach ensures a robust and production-ready architecture suitable for real-world data engineering workflows.

## Relevant commands to check status on container services

These commands help you manage your Docker containers, Airflow DAGs, DBT workflows, and Python virtual environments efficiently.

### Docker Commands

```bash
# Lists all running Docker containers
docker ps  # Shows currently running containers.
# Lists all Docker containers, including stopped ones
docker ps -a  # Shows all containers, whether running or stopped.
# Builds and starts the services defined in the docker-compose.yml file
docker-compose up --build  # Builds images (if needed) and starts the services.
# Builds and starts the services in detached mode (runs in the background)
docker-compose up --build -d  # Runs the services in detached mode, allowing you to continue using the terminal.
# Stops and removes containers, networks, and volumes defined in the docker-compose.yml file
docker-compose down -v  # Stops the services and removes all related containers, networks, and volumes.
```

### Airflow Commands

```bash
# Lists all the DAGs (Directed Acyclic Graphs) currently available in Airflow
docker exec -it airflow airflow dags list  # Shows all the DAGs currently available in the Airflow instance.
# Lists any errors encountered when importing DAGs into Airflow
docker exec -it airflow_webserver airflow dags list-import-errors  # Displays any errors that occurred during DAG import.
```

### DBT Commands

```bash
# Opens a bash shell inside the DBT container
docker exec -it dbt-container bash  # Starts a bash shell session inside the DBT container for interactive work.
# Runs a debug check in the DBT container to ensure your project is configured correctly
docker exec -it dbt-container dbt debug  # Executes a DBT debug check to verify the project configuration and environment.
```

#### DBT Workflow Commands

```bash
# Checks the DBT project's health and connectivity to the database
dbt debug  # Verifies the configuration of the DBT project, including database connection and environment settings.
# Compiles the DBT project, preparing it for execution
dbt compile  # Compiles the DBT project without running transformations, useful for checking the SQL queries before execution.
# Executes the DBT project, running the SQL transformations on the database
dbt run  # Runs the DBT project, executing the SQL models and transformations on the database.
```

### Python Virtual Environment Commands

```bash
# Creates a new virtual environment in the 'venv' directory
python -m venv venv  # Initializes a new Python virtual environment in the 'venv' directory.
# Activates the virtual environment on Windows
.\venv\Scripts\activate  # Activates the virtual environment on Windows (for Command Prompt or PowerShell).
# Deactivates the current Python virtual environment
deactivate  #